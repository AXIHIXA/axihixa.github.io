---
title: "LBMamba: Locally Bi-directional Mamba"
collection: publications
category: conferences
permalink: /publication/2025-05-19-LBMamba
excerpt: 'We present LBVim, a scalable vision backbone that alternates scan directions every two layers to recover a global receptive field without extra backward sweeps. Jingwei Zhang and <strong>Xi Han</strong> contributed equally to this paper.'
date: 2025-06-19
venue: 'arXiv preprint arXiv:2506.15976'
paperurl: 'https://arxiv.org/abs/2506.15976'
citation: 'Jingwei Zhang*, <strong>Xi Han*</strong>, Hong Qin, Mahdi S. Hosseini, and Dimitris Samaras, &quot;LBMamba: Locally Bi-directional Mamba&quot;, <i>arXiv preprint arXiv:2506.15976</i>, 2025.'
---

**Abstract**. Mamba, a State Space Model (SSM) that accelerates training by recasting recurrence as a parallel selective scan, has recently emerged as a linearly-scaling, efficient alternative to self-attention. Because of its unidirectional nature, each state in Mamba only has information of its previous states and is blind to states after. Current Mamba-based computer-vision methods typically overcome this limitation by augmenting Mamba's global forward scan with a global backward scan, forming a bi-directional scan that restores a full receptive field. However, this operation doubles the computational load, eroding much of the efficiency advantage that originally Mamba have. To eliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM block that embeds a lightweight locally backward scan inside the forward selective scan and executes it entirely in per-thread registers. Building on LBMamba, we present LBVim, a scalable vision backbone that alternates scan directions every two layers to recover a global receptive field without extra backward sweeps. We validate the versatility of our approach on both natural images and whole slide images (WSIs). We show that our LBVim constantly offers a superior performance-throughput trade-off. That is under the same throughput, LBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K classification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic segmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection dataset. We also integrate LBMamba into the SOTA pathology multiple instance learning (MIL) approach, MambaMIL, which uses single directional scan. Experiments on 3 public WSI classification datasets for show that our method achieves a relative improvement of up to 3.06% better AUC, 3.39% better F1, 1.67% better accuracy.

More information are available at [arXiv](https://arxiv.org/abs/2506.15976). 

---
title: "Pose2Seg: Detection Free Human Instance Segmentation"
collection: publications
category: conferences
permalink: /publication/2019-04-08-Pose2Seg
excerpt: 'Detection Free Human Instance Segmentation.'
date: 2019-04-08
venue: 'Proceedings of 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)'
paperurl: 'http://axihixa.github.io/files/paper/2019-CVPR-Pose2Seg-Paper.pdf'
citation: 'Song-Hai Zhang, Ruilong Li, Xin Dong, Paul Rosin, Zixi Cai, <strong>Xi Han</strong>, Dingcheng Yang, Haozhi Huang, and Shi-Min Hu, &quot;Pose2seg: Detection free human instance segmentation&quot;, In <i>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision And Pattern Recognition (CVPR)</i>, 2019.'
---

**Abstract**. The standard approach to image instance segmentation is to perform the object detection first, and then segment the object from the detection bounding-box. More recently, deep learning methods like Mask R-CNN perform them jointly. However, little research takes into account the uniqueness of the" human" category, which can be well defined by the pose skeleton. Moreover, the human pose skeleton can be used to better distinguish instances with heavy occlusion than using bounding-boxes. In this paper, we present a brand new pose-based instance segmentation framework for humans which separates instances based on human pose, rather than proposal region detection. We demonstrate that our pose-based framework can achieve better accuracy than the state-of-art detection-based approach on the human instance segmentation problem, and can moreover better handle occlusion. Furthermore, there are few public datasets containing many heavily occluded humans along with comprehensive annotations, which makes this a challenging problem seldom noticed by researchers. Therefore, in this paper we introduce a new benchmark" Occluded Human (OCHuman)", which focuses on occluded humans with comprehensive annotations including bounding-box, human pose and instance masks. This dataset contains 8110 detailed annotated human instances within 4731 images. With an average 0.67 MaxIoU for each person, OCHuman is the most complex and challenging dataset related to human instance segmentation. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study.

More information are available at [arXiv](https://arxiv.org/abs/1803.10683), [CVS Open Access](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Pose2Seg_Detection_Free_Human_Instance_Segmentation_CVPR_2019_paper.pdf), and [GitHub](https://github.com/liruilong940607/Pose2Seg). 
